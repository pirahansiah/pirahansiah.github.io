Week1 Introduction to Machine Learning

Applications of machine learning
Supervised learning
Unsupervised learning
Jupyter notebooks
Regression model
Linear regression model
Cost function
Gradient descent algorithm
Train the model with gradient descent

Week2 Regression with multiple input variables
Multiple features(variables)
Vectorization
Gradient descent for multiple linear regression
An alternative to gradient descent: normal equation
Feature scaling
Checking gradient descent for convergence
Automatic convergence test
Choosing the learning rate
Feature engineering
Using intuition to design new features, by transforming or combining existing features

Week3 Classification
Classification model
Logistic regression
Cost function
Gradient descent algorithm
Train the model with gradient descent
Evaluating the model
Accuracy
Precision
Recall
F1 score
ROC curve
AUC

Week4 Evaluating and improving machine learning models
Overfitting and underfitting
Regularization
Cross-validation
Ensemble learning
Bagging
Boosting
Adaboost
Random forest
XGBoost

Week5 Clustering
K-means clustering
Hierarchical clustering
DBSCAN
Anomaly detection

Week6 Natural language processing
Bag-of-words
TF-IDF
Word embeddings
Sentiment analysis
Machine translation

Week7 Reinforcement learning
Q-learning
Deep Q-learning

Week8 Deep learning
Artificial neural networks
Convolutional neural networks
Recurrent neural networks
Deep reinforcement learning














Feature scaling
to run faster
mean/average normalization
z-score normalization
Checking gradient descent for convergence
automatic convergence test
epsilon be 10^-3
if J decreases by < epsilon in one iteration
declare convergence
Choosing the learning rate
Feature engineering
using intuition to design new features, by transforming or combining original features
Polynomial regression
Logistic regression
class=category
0=false=no=negative class=absence
1=true=yes=positive class=presence
sigmoid function
logistic function
Decision boundary
linear decision boundaries
non-linear decision boundaries
Cost function for logistic regression
squared error cost
linear regression
convex
logistic regression
non-convex
logistic loss function
Simplified Cost Function for Logistic Regression
maximum likelihood
Gradient Descent Implementation
The problem of overfitting
underfit: does not fit the training set well; high bias
just right: fits training set pretty well : generalization
overfitting: fits the training set extremely well: high variance
Addressing overfitting
overfit
collect more training examples
select features to include/exclude
all features+insufficient data=overfit
less features=less likely to overfit
regularization
reduce the size of parameters; eliminate feature
lambda: regularization parameter (regularization term)
if lambda 0 =overfit
if lambda big=underfit
Regularized linear regression
Regularized logistic regression

regularization: reduce the size of parameters; eliminate feature
cost function with regularization:
simpler model
lambda: regularization parameter (regularization term)
if lambda 0 =overfit
if lambda big=underfit
Regularized linear regression
Regularized logistic regression


%%%%%%%%%%%%%****************** not finished 

