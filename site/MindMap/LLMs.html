<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.15.0/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.15.0/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.15.0/dist/index.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const {
    el
  } = markmap.Toolbar.create(mm);
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root, jsonOptions) => {
          const markmap = getMarkmap();
          window.mm = markmap.Markmap.create('svg#mindmap', (getOptions || markmap.deriveOptions)(jsonOptions), root);
        })(() => window.markmap,null,{"type":"heading","depth":0,"payload":{"lines":[0,1]},"content":"LLMs : reduce size &amp; increase speed","children":[{"type":"bullet_list","depth":1,"payload":{"lines":[1,10]},"content":"","children":[{"type":"list_item","depth":2,"payload":{"lines":[1,2]},"content":"Pruning","children":[{"type":"list_item","depth":3,"payload":{"lines":[2,3]},"content":"go for bigger size of network with many layers then pruning much better and faster","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[3,4]},"content":"model pruning: reducing redundant parameters which are not sensitive to the performance","children":[{"type":"list_item","depth":4,"payload":{"lines":[4,5]},"content":"aim: remove all connections with absolute weights below a threshold","children":[]}]}]},{"type":"list_item","depth":2,"payload":{"lines":[5,6]},"content":"Quantization","children":[{"type":"bullet_list","depth":3,"payload":{"lines":[6,7]},"content":"","children":[{"type":"list_item","depth":4,"payload":{"lines":[6,7]},"content":"The best way is using Google library which support most comprehensive methods","children":[]}]},{"type":"bullet_list","depth":3,"payload":{"lines":[7,10]},"content":"","children":[{"type":"list_item","depth":4,"payload":{"lines":[7,8]},"content":"compresses by reducing the number of bits used to represent the weights","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[8,9]},"content":"quantization effectively constraints the number of different weights we can use inside our kernels","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[9,10]},"content":"per-channel quantization for weights, which improves performance by model compression and latency reduction.","children":[]}]}]}]},{"type":"bullet_list","depth":1,"payload":{"lines":[10,79]},"content":"","children":[{"type":"list_item","depth":2,"payload":{"lines":[10,11]},"content":"Ensemble Methods","children":[{"type":"list_item","depth":3,"payload":{"lines":[11,12]},"content":"<strong>bagging</strong> is a type of ensemble method where multiple models are trained in parallel on subsampled datasets (reduces error due to variance);","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[12,13]},"content":"feature-based, compact representation, easy to reduce model size, model size can be reduced post-training","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[13,14]},"content":"<strong>boosting</strong> is a type of ensemble method where multiple models are trained in sequence to improve upon the errors of the previous model (reduces error due to bias)","children":[]}]},{"type":"list_item","depth":2,"payload":{"lines":[14,15]},"content":"Distillation Techniques","children":[{"type":"list_item","depth":3,"payload":{"lines":[15,16]},"content":"Distill-Net: Application-Specific Distillation of Deep Convolutional Neural Networks","children":[{"type":"list_item","depth":4,"payload":{"lines":[16,17]},"content":"for Resource-Constrained IoT Platforms","children":[]}]}]},{"type":"list_item","depth":2,"payload":{"lines":[17,18]},"content":"Binarized Neural Networks (BNNs)","children":[{"type":"list_item","depth":3,"payload":{"lines":[18,19]},"content":"It is not support by GPU hardware such as Jetson Nano","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[19,20]},"content":"mostly based on CPU","children":[]}]},{"type":"list_item","depth":2,"payload":{"lines":[20,21]},"content":"Apache TVM","children":[{"type":"list_item","depth":3,"payload":{"lines":[21,22]},"content":"TVM (incubating) is a compiler stack for deep learning systems","children":[{"type":"list_item","depth":4,"payload":{"lines":[22,23]},"content":"challenges with large scale models","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[23,24]},"content":"deep neural networks are:","children":[{"type":"list_item","depth":5,"payload":{"lines":[24,25]},"content":"expensive","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[25,26]},"content":"computationally expensive","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[26,27]},"content":"memory intensive","children":[]}]},{"type":"list_item","depth":4,"payload":{"lines":[27,28]},"content":"hindering their deployment in:","children":[{"type":"list_item","depth":5,"payload":{"lines":[28,29]},"content":"devices with low memory resources","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[29,30]},"content":"applications with strict latency requirements","children":[]}]},{"type":"list_item","depth":4,"payload":{"lines":[30,31]},"content":"other issues:","children":[{"type":"list_item","depth":5,"payload":{"lines":[31,32]},"content":"data security: tend to memorize everything including PII","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[32,33]},"content":"bias e.g. profanity: trained on large scale public data","children":[]}]},{"type":"list_item","depth":4,"payload":{"lines":[33,34]},"content":"self discovering: instead of manually configuring conversational flows, automatically discover them from your data","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[34,35]},"content":"self training: let your system train itself with new examples","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[35,36]},"content":"self managing: let your system optimize by itself","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[36,37]},"content":"knowledge distillation","children":[]}]}]},{"type":"list_item","depth":2,"payload":{"lines":[37,38]},"content":"Distributed machine learning and load balancing strategy","children":[{"type":"list_item","depth":3,"payload":{"lines":[38,39]},"content":"run models which use all processing power like CPU,GPU,DSP,AI chip together to enhance inference performance","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[39,40]},"content":"dynamic pruning of kernels which aims to the parsimonious inference by learning to exploit and dynamically remove the redundant capacity of a CNN architecture.","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[40,41]},"content":"partitioning techniques through convolution layer fusion to dynamically select the optimal partition according to the availability of computational resources and network conditions","children":[]}]},{"type":"list_item","depth":2,"payload":{"lines":[41,42]},"content":"parallel computing","children":[{"type":"list_item","depth":3,"payload":{"lines":[42,43]},"content":"ffmpeg","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[43,44]},"content":"GStreamer","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[44,45]},"content":"celery","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[45,46]},"content":"GPU library for python: PyCUDA, NumbaPro, PyOpenCL, CuPy","children":[]}]},{"type":"list_item","depth":2,"payload":{"lines":[46,47]},"content":"Low rank matrix factorization (LRMF)","children":[{"type":"list_item","depth":3,"payload":{"lines":[47,48]},"content":"there exists latent structures in the data, by uncovering which we can obtain a compressed representation of the data","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[48,49]},"content":"LRMF factorizes the original matrix into lower rank matrices while preserving latent structures and addressing the issue of sparseness","children":[]}]},{"type":"list_item","depth":2,"payload":{"lines":[49,50]},"content":"Compact convolutional filters (Video/CNN)","children":[{"type":"list_item","depth":3,"payload":{"lines":[50,51]},"content":"designing special structural convolutional filters to save parameters","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[51,52]},"content":"replace over parametric filters with compact filters to achieve overall speedup while maintaining comparable accuracy","children":[]}]},{"type":"list_item","depth":2,"payload":{"lines":[52,53]},"content":"Knowledge distillation","children":[{"type":"list_item","depth":3,"payload":{"lines":[53,54]},"content":"training a compact neural network with distilled knowledge of a large model","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[54,55]},"content":"distillation (knowledge transfer) from an ensemble of big networks into a much smaller network which learns directly from the cumbersome model's outputs, that is lighter to deploy","children":[]}]},{"type":"list_item","depth":2,"payload":{"lines":[55,56]},"content":"Neural Networks Compression Framework (NNCF)","children":[]},{"type":"list_item","depth":2,"payload":{"lines":[56,57]},"content":"if the object is large and we do not need small anchor","children":[{"type":"list_item","depth":3,"payload":{"lines":[57,58]},"content":"decrease size of image input but reduce the accuracy","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[58,59]},"content":"in mobileNet we can remove small part of network which related to small objects","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[59,60]},"content":"in YOLO reduce number of anchor","children":[]}]},{"type":"list_item","depth":2,"payload":{"lines":[60,61]},"content":"frameworks","children":[{"type":"list_item","depth":3,"payload":{"lines":[61,62]},"content":"CoreML","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[62,63]},"content":"ML kit","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[63,64]},"content":"FRITZ","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[64,65]},"content":"PyTorch Lightning","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[65,66]},"content":"PyTorch Mobile","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[66,67]},"content":"TensorFlow Lite","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[67,68]},"content":"TensorFlow.js","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[68,69]},"content":"NVIDIA TensorRT","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[69,70]},"content":"IntelÂ® Distribution of OpenVINO Toolkit","children":[]},{"type":"list_item","depth":3,"payload":{"lines":[70,71]},"content":"tinyML","children":[{"type":"list_item","depth":4,"payload":{"lines":[71,72]},"content":"enabling ultra-low power","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[72,73]},"content":"machine learning at the edge","children":[]},{"type":"list_item","depth":4,"payload":{"lines":[73,74]},"content":"tiny machine learning with Arduino","children":[{"type":"list_item","depth":5,"payload":{"lines":[74,75]},"content":"TensorFlow Lite on Microcontroller","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[75,76]},"content":"Gesture Recognition","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[76,77]},"content":"OpenMV/Tensorflow/","children":[]},{"type":"list_item","depth":5,"payload":{"lines":[77,78]},"content":"studio.edgeimpulse.com","children":[]}]}]},{"type":"list_item","depth":3,"payload":{"lines":[78,79]},"content":"MediaPipe","children":[]}]}]}]},{})</script>
</body>
</html>
