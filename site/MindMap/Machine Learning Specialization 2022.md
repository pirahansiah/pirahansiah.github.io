# Machine <br /> Learning  <br /> Specialization <br /> 2022

## Supervised <br /> Machine<br /> Learning<br />  Regression <br />&Classification

* week 1: https://www.tiziran.com/
* week 2: https://www.tiziran.com/
* week 3: https://www.tiziran.com/

## Advanced <br /> Learning <br /> Algorithms

### Week 1

* why now?
* demand prediction
* input layer; hidden layer (activation); output layer
* Face recognition - car classification 
* Neural network layer
* inference: making predictions (forward propagation)
* inference in code (TF)**
* Data in TensorFlow & numpy **
* Building a neural network
* Forward prop in a single layer
* forward prop (Coffeee roasting model)
* propagation (forward prop in NumPy)
* AGI
	* ANI: artificial narrow intelligence
	* AGI: artificial general intelligence
	* one learning algorithm
* vectorization (NN implemented efficiently)
* matrix multiplication rules & code


### Week 2
* TensorFlow implementation #code
* Training Details #code
	* model training steps
		* how compute output
		* specify loss and cost
		* train on data to minimize cost
			* create the model: units and activation 
			* loss and cost functions: loss= BinaryCrossentropy | MeanSquaredError
			* minimize cost function "Gradient descent":  
* Alternatives to the sigmoid activation 
	* Linear activation function
	* Sigmoid
	* ReLU
	* Softmax activation 
* Choosing activation functions
	* binary classification -> Sigmoid (slower)
	* Regression -> Linear activation function
	* Regression (not negative value) -> ReLU (faster) 
* Why do we need activation functions?
	* use ReLU
* Multiclass classification
* Softmax
* Neural Network with Softmax output
	* SparseCategoricalCrossentropy
* Improved implementation of softmax  #code
	* loss=SparseCrossEntropy(from_logits=True)
* Classification with multiple outputs
	* multi-label classification
* Advanced Optimization
	* Adam: Adaptive Moment estimation 
* Additional Layer Types
	* Convolutional Layer
		* Faster computation
		* need less training data (less prone to over-fitting)
* 

### Week 3
* a
* b
* 
