# LLM & DL optimization for computer vision

- Hardware-Aware Optimization 
    - Optimizing models for specific hardware platforms by leveraging techniques like hardware co-design and exploiting hardware accelerators.
- Energy-Efficient Training and Inference
    - Developing algorithms and tools that minimize the energy consumption during training and deploying large language models.
- Parameter/Model Pruning
    - This involves removing unnecessary or redundant parameters from a neural network that do not contribute significantly to the output. 
    - It helps reduce the size of the model and its computational complexity.
- Quantization
    - This process reduces the precision of the weights and activations of models from floating-point to lower-bit integers, decreasing the model size and speeding up inference with minimal loss in accuracy.
- Binarized Neural Networks (BNNs)
    - These networks use binary values for activations and weights instead of floating-point numbers, significantly reducing memory usage and computational requirements.
- Low Rank Matrix Factorization (LRMF)
    - This technique decomposes matrices into lower rank approximations, reducing the number of parameters in fully connected layers and thus the computational complexity.
- Compact Convolutional Filters
    - This involves designing or using convolutional filters that require fewer parameters and operations, maintaining performance while reducing computational load.
- Knowledge Distillation
    - A smaller model (student) is trained to mimic a larger, pre-trained model (teacher). This allows the smaller model to achieve high accuracy while being more efficient.
- Adaptive Sparsity
    - This technique dynamically adjusts the level of pruning during training, allowing for a more granular control over the trade-off between model size and accuracy.
- Architecture Search
    - Utilizing reinforcement learning or evolutionary algorithms to automatically search for optimal network architectures that are efficient and accurate for a specific task.
- Code Size Optimizations for Embedded Systems
    - Techniques and strategies to reduce the code size of applications, especially for embedded systems. 
    - This includes compiler optimizations and software engineering techniques to manage the trade-offs between code size and performance, and specific compiler flags to reduce binary size
- Optimizing Machine Learning Models for IoT Applications
    - Approaches to make ML/AI applications more efficient in embedded or constrained environments, focusing on low latency, low bandwidth, and low power consumption. Strategies include structuring code effectively, model size reduction post-training, and ensemble methods for compact model representation
- Optimized Model Deployment
    - Strategies for deploying models efficiently using web frameworks, cloud services, Kubernetes for scalable applications, TensorFlow/PyTorch serving for optimized model serving, and Apache TVM for compiling deep learning systems for improved deployment efficiency