{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farshid Pirahansiah\n",
    "# Config jupyter lab for computer vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: widget. Using ipympl instead.\n"
     ]
    }
   ],
   "source": [
    "#!pip install line_profiler\n",
    "#conda install -c conda-forge ipympl\n",
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%load_ext line_profiler\n",
    "#%run\n",
    "#%pdb #Automatically enters the Python debugger after any uncaught exception. A great tool for debugging.\n",
    "#%prun #Runs code with the Python profiler to identify which internal parts of the function are taking the longest to execute.\n",
    "#%%capture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "\"\n",
    "<span style=\"color: red;\">2024.</span>\n",
    "<span style=\"color: green;\">Edge-based Generative AI for Image Processing Aplication</span>\n",
    "On-device processing: Prioritizing privacy and accessibility, LLMs on-device processing, ensuring that your data remains on your device. This not only enhances security but also has the potential to minimize latency, contributing to a more efficient user experience.\n",
    "\n",
    "Generative AI models are a class of AI that can generate new data that resembles existing data. They include foundation models that can be fine-tuned for different tasks, Variational Autoencoders (VAEs) that reduce the dimensionality of data and Generative Adversarial Networks (GANs) that use competing networks to generate realistic samples. Other models include Transformer-based models that use attention mechanisms to model long-term text dependencies, diffusion models that address information decay by removing noise in the latent space, and multimodal large language models (LLMs) that integrate image and text understanding.\n",
    "In summary, generative AI models are a class of AI that can generate new data that resembles existing data. They include foundation models, VAEs, GANs, Transformer-based models, diffusion models, and multimodal LLMs.\n",
    "\n",
    "# Farshid PirahanSiah\n",
    "## Generative AI models on Edge with on-device training \n",
    "- CNN, RNN, Transformer-based models, LLMs (GPT, )\n",
    "- Generative AI models\n",
    "    - foundation model\n",
    "        - train on huge data -> adapt to applications\n",
    "        - GPT, CLIP, DALL-E, \n",
    "    - Variational Autoencoders (VAEs)\n",
    "        - rapidly reduce the dimensionality of samples\n",
    "        - input (image) -> encoder -> latent space -> decoder -> reconstructed output (image)\n",
    "        - use for \n",
    "            - image synthesis\n",
    "            - data compression\n",
    "            - anomaly detection\n",
    "        - **Standard VAEs**\n",
    "            - **Beta-VAE**: Balances latent capacity and reconstruction.\n",
    "        - **Conditional VAEs (CVAEs)**\n",
    "            - **Conditional Beta-VAE**: Adds conditional variables for targeted generation.\n",
    "        - **Hierarchical VAEs**\n",
    "            - **NVAE**: Deep hierarchy, residual connections for stable high-quality output.\n",
    "    - Generative Adversarial Networks (GANs)\n",
    "        - use competing networks to produce realistic samples\n",
    "        - generator ; discriminator\n",
    "        - GANs in finance, spaceGAN (geospatial data), styleGAN2 (create video game characters)\n",
    "        - **Convolutional GANs**\n",
    "            - **DCGAN**: Deep convolutional networks for stable, quality generation.\n",
    "        - **Style-based GANs**\n",
    "            - **StyleGAN**: Fine-grained style control, realistic faces.\n",
    "            - **StyleGAN2**: Redesigns normalization, reduces artifacts.\n",
    "        - **Progressive Growing GANs**\n",
    "            - **PGGAN**: Increases network size during training for high-resolution output.\n",
    "    - Transformer-based Models\n",
    "        - use attention mechanisms to model long-term text dependencies\n",
    "        - **Autoregressive Models**\n",
    "            - **GPT-3**: Predictive text generation based on input.\n",
    "            - **Image GPT (iGPT)**: Generates images pixel by pixel.\n",
    "        - **Multimodal Transformers**\n",
    "            - **DALL-E**: Generates images from text descriptions.\n",
    "    - Diffusion Models\n",
    "        - address information decay by removing noise in the latent space\n",
    "        - step 1: forward diffusion to add random noise to the data; \n",
    "        - step 2: reverse diffusion: turn the noise , recover data, generate the desired output\n",
    "        - **Basic Diffusion Models**\n",
    "            - **DDPM**: Generates data from noise by reversing diffusion.\n",
    "        - **Advanced Diffusion Techniques**\n",
    "            - **Improved DDPM**: More efficient, improved sampling.\n",
    "            - **Guided Diffusion**: Produces specific outputs with classifier guidance.\n",
    "    - Multimodal Large Language Models (LLMs)\n",
    "        - **CLIP**\n",
    "            - Integrates image and text understanding.\n",
    "        - **Perceiver IO**\n",
    "            - Handles audio, visual, text with a generalized architecture.\n",
    "        - **FLAVA**\n",
    "            - Self-supervised, unified model for vision, language, and multimodal tasks.\n",
    "- LLMOps: MLOps Tools: MLflow and Hugging Face    \n",
    "    - <span style=\"color: green; font-weight: bold; font-style: italic; white-space: nowrap;\"> ðŸ“š ðŸ¤” MLflow <span style=\"display: inline-block; animation: slide 2s infinite linear;\">â†’</span> </span>  <style>            @keyframes slide {            0% { transform: translateX(0); }            100% { transform:translateX(20px); }            }            </style>\n",
    " \n",
    "        - MLflow Tracking - Logs key metrics, parameters, models, and other artifacts when running ML code to monitor experiments\n",
    "        - MLflow Projects - Configurable standard format for organizing ML code to ensure consistency and reproducibility\n",
    "        - MLflow Models - Package ML model files with their dependencies so they can be deployed on diverse platforms\n",
    "        - pip install mlflow\n",
    "        - mlflow ui\n",
    "        - mlflow experiments create --experiment-name metrics-test ==> id 2\n",
    "        - MLFLOW_EXPERIMENT_ID=2 python test-mlflow.py\n",
    "        - load_model= mlflow.pyfunc.load_model(logged_model)\n",
    "        - mlflow run . -P filename=inputfile\n",
    "        - hugging face \n",
    "            - models, dataset, spaces\n",
    "            - streamlit, gradio\n",
    "            - pip install huggingface_hub\n",
    "            - huggingface-cli login\n",
    "            - git lfs install\n",
    "            - git clone <model url>\n",
    "            - \n",
    "        - \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Large Vision Models (LVMs)\n",
    "-  Stable Diffusion\n",
    "    - Why Stable Diffusion \n",
    "        - **Brainstorming and Ideation:** Rapidly generate images to explore various design layouts, scenes, or concepts. Itâ€™s an efficient tool for experimenting with creative ideas, such as different settings or stylized visual effects, providing more specific and customized placeholders than stock photos or traditional sketches.\n",
    "        - **Character Design:** Whether for video games, animations, or comic books, Stable Diffusion can help conceptualize and visualize characters, aiding in the development and refinement of unique designs.\n",
    "        - **Storyboarding:** Create visual narratives for video productions, movies, or advertisements. Stable Diffusion can generate scenes and sequences, facilitating the storytelling process and the visualization of cinematic ideas.\n",
    "        - **Communication:** Enhance presentations or proposals with high-quality visualizations. For office managers, interior designers, or event planners, itâ€™s a tool to convey ideas about space layouts, mood, atmosphere, and color schemes effectively, even if the images arenâ€™t perfect replicas.\n",
    "        - **Asset Generation:** Produce elements for compositing into photographs or artworks, create textures for 3D modeling, or develop backgrounds for motion graphics. Itâ€™s a versatile tool for augmenting traditional and digital art processes with unique visual components.\n",
    "        - **Fun and Exploration:** Beyond professional applications, Stable Diffusion offers a platform for fun and creative exploration. It encourages playful experimentation with images and concepts, often leading to unexpected and delightful results.  \n",
    "    - **Stable Diffusion**\n",
    "        - **Image Style**\n",
    "            - Blends photorealism and illustration\n",
    "            - Customizable styles via data models\n",
    "        - **Data Models**\n",
    "            - Open-source, community-driven enhancements\n",
    "            - Customizable for content safety or style\n",
    "        - **Accessibility**\n",
    "            - Free and open-source\n",
    "            - Standalone applications\n",
    "            - Integrates with software like Photoshop\n",
    "        - **Ethics**\n",
    "            - Training on diverse datasets\n",
    "            - Ethical considerations similar to competitors\n",
    "        - **Features**\n",
    "            - Image-to-image generation\n",
    "            - Custom training for specific recognition\n",
    "        - **Compared to Others**\n",
    "            - DALL-E and Midjourney focus on photorealism\n",
    "            - Adobe Firefly trained on ethical datasets\n",
    "            - Midjourney accessible via Discord\n",
    "            - Others require payment, Stable Diffusion is free\n",
    "        - **Community Impact**\n",
    "            - Vibrant open-source community\n",
    "            - Continuous improvement and feature additions\n",
    "        - **Technology**\n",
    "            - AI image generator revolution\n",
    "            - Different from traditional digital tools\n",
    "            - Democratizes sophisticated image generation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/led-large-16384-arxiv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# If you are running on a GPU, you can uncomment the device parameter to utilize it\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3523\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3524\u001b[0m     (\n\u001b[1;32m   3525\u001b[0m         model,\n\u001b[1;32m   3526\u001b[0m         missing_keys,\n\u001b[1;32m   3527\u001b[0m         unexpected_keys,\n\u001b[1;32m   3528\u001b[0m         mismatched_keys,\n\u001b[1;32m   3529\u001b[0m         offload_index,\n\u001b[1;32m   3530\u001b[0m         error_msgs,\n\u001b[0;32m-> 3531\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3538\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3542\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3543\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3549\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:3904\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   3896\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3897\u001b[0m         state_dict,\n\u001b[1;32m   3898\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3902\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   3903\u001b[0m     )\n\u001b[0;32m-> 3904\u001b[0m     error_msgs \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3905\u001b[0m     offload_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3907\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   3908\u001b[0m \n\u001b[1;32m   3909\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:628\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 628\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:626\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:626\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 626 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:626\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:622\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    620\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_param\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"allenai/led-large-16384-arxiv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# If you are running on a GPU, you can uncomment the device parameter to utilize it\n",
    "# pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "long_text = text\n",
    "generated_text = pipe(\n",
    "    long_text, \n",
    "    truncation=True, \n",
    "    max_length=1024,  # Adjusted to a higher value for potentially longer outputs\n",
    "    no_repeat_ngram_size=5, \n",
    "    num_beams=3, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Printing the generated text to see the output\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input ids are automatically padded from 2756 to 3072 to be a multiple of `config.attention_window`: 1024\n",
    "[{'generated_text': ' generative models are a class of artificial intelligence ( AI ) that can generate new data that resembles existing data. \\n they include foundation models that can be fine-tuned for different tasks , Variational Autoencoders (VAEs ) that reduce the dimensionality of data and Generative Adversarial Networks (GANs ) that use competing networks to generate realistic samples . \\n other models include Transformer-based models that use attention mechanisms to model long-term text dependencies , diffusion models that address information decay by removing noise in the latent space , and multimodal large language models (LLMs ) that integrate image and text understanding.    \\n generative models on Edge with on-device training - CNN, RNN , Transformer-based model , LLMs (GPT, )- Generative models on Edge  - foundation model , on-device training , on-device processing  - image synthesis , image compression , anomaly detection , data compression     * keywords : * generative models , image processing , image synthesis , image synthesis , data compression , data compression , anomaly detection '}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index\n",
    "#!pip install llama-index-embeddings-huggingface\n",
    "#!pip install llama-index-embeddings-instructor\n",
    "#!pip install llama-index-llms-ollama\n",
    "#!pip install llama-index\n",
    "input_dir_path='/Users/farshid/farshid/pirahansiah.github.io/src/books/goal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, there are several methods discussed for setting and achieving goals. Here are some of them:\n",
      "\n",
      "1. FAST Goals: This method emphasizes adaptability, transparency, and continuous improvement. It involves regular discussions, articulating goals clearly, maintaining transparency, and ensuring challenges yet achievable.\n",
      "2. Backward Goal Setting: This approach starts with a clear vision of the final goal and works backward to create a reverse timeline. It's ideal for achieving complex and long-term goals.\n",
      "\n",
      "Additionally, other methods mentioned include:\n",
      "\n",
      "1. SMART Goals: This framework involves setting specific, measurable, achievable, relevant, and time-bound goals.\n",
      "2. GROW Model: This model stands for Goal, Reality, Options, and Will, providing a structured approach to goal setting and problem-solving.\n",
      "\n",
      "Please let me know if you'd like me to elaborate on any of these methods or if there's anything else I can help with!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, PromptTemplate\n",
    "\n",
    "# load data\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# ====== Create vector store and upload indexed data ======\n",
    "Settings.embed_model = embed_model # we specify the embedding model to be used\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# setting up the llm\n",
    "llm = Ollama(model=\"llama3\", request_timeout=820.0) \n",
    "\n",
    "# ====== Setup a query engine on the index previously created ======\n",
    "Settings.llm = llm # specifying the llm to be used\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)\n",
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "response = query_engine.query('What are the methods for better reach to goal?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, I can confidently answer that:\n",
      "\n",
      "SMART stands for Specific, Measurable, Achievable, Relevant, and Time-bound. It's a framework used to set clear and actionable goals, introduced by George T. Doran in 1981.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is SMART?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('index_data.pkl', 'wb') as file:\n",
    "    pickle.dump(index, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('farshid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
