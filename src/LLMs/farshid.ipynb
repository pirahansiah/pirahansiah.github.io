{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farshid Pirahansiah\n",
    "# Config jupyter lab for computer vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: widget. Using ipympl instead.\n"
     ]
    }
   ],
   "source": [
    "#!pip install line_profiler\n",
    "#conda install -c conda-forge ipympl\n",
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%load_ext line_profiler\n",
    "#%run\n",
    "#%pdb #Automatically enters the Python debugger after any uncaught exception. A great tool for debugging.\n",
    "#%prun #Runs code with the Python profiler to identify which internal parts of the function are taking the longest to execute.\n",
    "#%%capture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "\"\n",
    "<span style=\"color: red;\">2024.</span>\n",
    "<span style=\"color: green;\">Edge-based Generative AI for Image Processing Aplication</span>\n",
    "On-device processing: Prioritizing privacy and accessibility, LLMs on-device processing, ensuring that your data remains on your device. This not only enhances security but also has the potential to minimize latency, contributing to a more efficient user experience.\n",
    "\n",
    "Generative AI models are a class of AI that can generate new data that resembles existing data. They include foundation models that can be fine-tuned for different tasks, Variational Autoencoders (VAEs) that reduce the dimensionality of data and Generative Adversarial Networks (GANs) that use competing networks to generate realistic samples. Other models include Transformer-based models that use attention mechanisms to model long-term text dependencies, diffusion models that address information decay by removing noise in the latent space, and multimodal large language models (LLMs) that integrate image and text understanding.\n",
    "In summary, generative AI models are a class of AI that can generate new data that resembles existing data. They include foundation models, VAEs, GANs, Transformer-based models, diffusion models, and multimodal LLMs.\n",
    "\n",
    "# Farshid PirahanSiah\n",
    "## Generative AI models on Edge with on-device training \n",
    "- CNN, RNN, Transformer-based models, LLMs (GPT, )\n",
    "- Generative AI models\n",
    "    - foundation model\n",
    "        - train on huge data -> adapt to applications\n",
    "        - GPT, CLIP, DALL-E, \n",
    "    - Variational Autoencoders (VAEs)\n",
    "        - rapidly reduce the dimensionality of samples\n",
    "        - input (image) -> encoder -> latent space -> decoder -> reconstructed output (image)\n",
    "        - use for \n",
    "            - image synthesis\n",
    "            - data compression\n",
    "            - anomaly detection\n",
    "        - **Standard VAEs**\n",
    "            - **Beta-VAE**: Balances latent capacity and reconstruction.\n",
    "        - **Conditional VAEs (CVAEs)**\n",
    "            - **Conditional Beta-VAE**: Adds conditional variables for targeted generation.\n",
    "        - **Hierarchical VAEs**\n",
    "            - **NVAE**: Deep hierarchy, residual connections for stable high-quality output.\n",
    "    - Generative Adversarial Networks (GANs)\n",
    "        - use competing networks to produce realistic samples\n",
    "        - generator ; discriminator\n",
    "        - GANs in finance, spaceGAN (geospatial data), styleGAN2 (create video game characters)\n",
    "        - **Convolutional GANs**\n",
    "            - **DCGAN**: Deep convolutional networks for stable, quality generation.\n",
    "        - **Style-based GANs**\n",
    "            - **StyleGAN**: Fine-grained style control, realistic faces.\n",
    "            - **StyleGAN2**: Redesigns normalization, reduces artifacts.\n",
    "        - **Progressive Growing GANs**\n",
    "            - **PGGAN**: Increases network size during training for high-resolution output.\n",
    "    - Transformer-based Models\n",
    "        - use attention mechanisms to model long-term text dependencies\n",
    "        - **Autoregressive Models**\n",
    "            - **GPT-3**: Predictive text generation based on input.\n",
    "            - **Image GPT (iGPT)**: Generates images pixel by pixel.\n",
    "        - **Multimodal Transformers**\n",
    "            - **DALL-E**: Generates images from text descriptions.\n",
    "    - Diffusion Models\n",
    "        - address information decay by removing noise in the latent space\n",
    "        - step 1: forward diffusion to add random noise to the data; \n",
    "        - step 2: reverse diffusion: turn the noise , recover data, generate the desired output\n",
    "        - **Basic Diffusion Models**\n",
    "            - **DDPM**: Generates data from noise by reversing diffusion.\n",
    "        - **Advanced Diffusion Techniques**\n",
    "            - **Improved DDPM**: More efficient, improved sampling.\n",
    "            - **Guided Diffusion**: Produces specific outputs with classifier guidance.\n",
    "    - Multimodal Large Language Models (LLMs)\n",
    "        - **CLIP**\n",
    "            - Integrates image and text understanding.\n",
    "        - **Perceiver IO**\n",
    "            - Handles audio, visual, text with a generalized architecture.\n",
    "        - **FLAVA**\n",
    "            - Self-supervised, unified model for vision, language, and multimodal tasks.\n",
    "- LLMOps: MLOps Tools: MLflow and Hugging Face    \n",
    "    - <span style=\"color: green; font-weight: bold; font-style: italic; white-space: nowrap;\"> ðŸ“š ðŸ¤” MLflow <span style=\"display: inline-block; animation: slide 2s infinite linear;\">â†’</span> </span>  <style>            @keyframes slide {            0% { transform: translateX(0); }            100% { transform:translateX(20px); }            }            </style>\n",
    " \n",
    "        - MLflow Tracking - Logs key metrics, parameters, models, and other artifacts when running ML code to monitor experiments\n",
    "        - MLflow Projects - Configurable standard format for organizing ML code to ensure consistency and reproducibility\n",
    "        - MLflow Models - Package ML model files with their dependencies so they can be deployed on diverse platforms\n",
    "        - pip install mlflow\n",
    "        - mlflow ui\n",
    "        - mlflow experiments create --experiment-name metrics-test ==> id 2\n",
    "        - MLFLOW_EXPERIMENT_ID=2 python test-mlflow.py\n",
    "        - load_model= mlflow.pyfunc.load_model(logged_model)\n",
    "        - mlflow run . -P filename=inputfile\n",
    "        - hugging face \n",
    "            - models, dataset, spaces\n",
    "            - streamlit, gradio\n",
    "            - pip install huggingface_hub\n",
    "            - huggingface-cli login\n",
    "            - git lfs install\n",
    "            - git clone <model url>\n",
    "            - \n",
    "        - \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Large Vision Models (LVMs)\n",
    "-  Stable Diffusion\n",
    "    - Why Stable Diffusion \n",
    "        - **Brainstorming and Ideation:** Rapidly generate images to explore various design layouts, scenes, or concepts. Itâ€™s an efficient tool for experimenting with creative ideas, such as different settings or stylized visual effects, providing more specific and customized placeholders than stock photos or traditional sketches.\n",
    "        - **Character Design:** Whether for video games, animations, or comic books, Stable Diffusion can help conceptualize and visualize characters, aiding in the development and refinement of unique designs.\n",
    "        - **Storyboarding:** Create visual narratives for video productions, movies, or advertisements. Stable Diffusion can generate scenes and sequences, facilitating the storytelling process and the visualization of cinematic ideas.\n",
    "        - **Communication:** Enhance presentations or proposals with high-quality visualizations. For office managers, interior designers, or event planners, itâ€™s a tool to convey ideas about space layouts, mood, atmosphere, and color schemes effectively, even if the images arenâ€™t perfect replicas.\n",
    "        - **Asset Generation:** Produce elements for compositing into photographs or artworks, create textures for 3D modeling, or develop backgrounds for motion graphics. Itâ€™s a versatile tool for augmenting traditional and digital art processes with unique visual components.\n",
    "        - **Fun and Exploration:** Beyond professional applications, Stable Diffusion offers a platform for fun and creative exploration. It encourages playful experimentation with images and concepts, often leading to unexpected and delightful results.  \n",
    "    - **Stable Diffusion**\n",
    "        - **Image Style**\n",
    "            - Blends photorealism and illustration\n",
    "            - Customizable styles via data models\n",
    "        - **Data Models**\n",
    "            - Open-source, community-driven enhancements\n",
    "            - Customizable for content safety or style\n",
    "        - **Accessibility**\n",
    "            - Free and open-source\n",
    "            - Standalone applications\n",
    "            - Integrates with software like Photoshop\n",
    "        - **Ethics**\n",
    "            - Training on diverse datasets\n",
    "            - Ethical considerations similar to competitors\n",
    "        - **Features**\n",
    "            - Image-to-image generation\n",
    "            - Custom training for specific recognition\n",
    "        - **Compared to Others**\n",
    "            - DALL-E and Midjourney focus on photorealism\n",
    "            - Adobe Firefly trained on ethical datasets\n",
    "            - Midjourney accessible via Discord\n",
    "            - Others require payment, Stable Diffusion is free\n",
    "        - **Community Impact**\n",
    "            - Vibrant open-source community\n",
    "            - Continuous improvement and feature additions\n",
    "        - **Technology**\n",
    "            - AI image generator revolution\n",
    "            - Different from traditional digital tools\n",
    "            - Democratizes sophisticated image generation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 2756 to 3072 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' generative models are a class of artificial intelligence ( AI ) that can generate new data that resembles existing data. \\n they include foundation models that can be fine-tuned for different tasks , Variational Autoencoders (VAEs ) that reduce the dimensionality of data and Generative Adversarial Networks (GANs ) that use competing networks to generate realistic samples . \\n other models include Transformer-based models that use attention mechanisms to model long-term text dependencies , diffusion models that address information decay by removing noise in the latent space , and multimodal large language models (LLMs ) that integrate image and text understanding.    \\n generative models on Edge with on-device training - CNN, RNN , Transformer-based model , LLMs (GPT, )- Generative models on Edge  - foundation model , on-device training , on-device processing  - image synthesis , image compression , anomaly detection , data compression     * keywords : * generative models , image processing , image synthesis , image synthesis , data compression , data compression , anomaly detection '}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"allenai/led-large-16384-arxiv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# If you are running on a GPU, you can uncomment the device parameter to utilize it\n",
    "# pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "long_text = text\n",
    "generated_text = pipe(\n",
    "    long_text, \n",
    "    truncation=True, \n",
    "    max_length=512,  # Adjusted to a higher value for potentially longer outputs\n",
    "    no_repeat_ngram_size=5, \n",
    "    num_beams=3, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Printing the generated text to see the output\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input ids are automatically padded from 2756 to 3072 to be a multiple of `config.attention_window`: 1024\n",
    "[{'generated_text': ' generative models are a class of artificial intelligence ( AI ) that can generate new data that resembles existing data. \\n they include foundation models that can be fine-tuned for different tasks , Variational Autoencoders (VAEs ) that reduce the dimensionality of data and Generative Adversarial Networks (GANs ) that use competing networks to generate realistic samples . \\n other models include Transformer-based models that use attention mechanisms to model long-term text dependencies , diffusion models that address information decay by removing noise in the latent space , and multimodal large language models (LLMs ) that integrate image and text understanding.    \\n generative models on Edge with on-device training - CNN, RNN , Transformer-based model , LLMs (GPT, )- Generative models on Edge  - foundation model , on-device training , on-device processing  - image synthesis , image compression , anomaly detection , data compression     * keywords : * generative models , image processing , image synthesis , image synthesis , data compression , data compression , anomaly detection '}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
