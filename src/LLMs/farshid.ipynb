{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Farshid Pirahansiah\n",
    "# Config jupyter lab for computer vision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: widget. Using ipympl instead.\n"
     ]
    }
   ],
   "source": [
    "#!pip install line_profiler\n",
    "#conda install -c conda-forge ipympl\n",
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "#%load_ext line_profiler\n",
    "#%run\n",
    "#%pdb #Automatically enters the Python debugger after any uncaught exception. A great tool for debugging.\n",
    "#%prun #Runs code with the Python profiler to identify which internal parts of the function are taking the longest to execute.\n",
    "#%%capture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "\"\n",
    "<span style=\"color: red;\">2024.</span>\n",
    "<span style=\"color: green;\">Edge-based Generative AI for Image Processing Aplication</span>\n",
    "On-device processing: Prioritizing privacy and accessibility, LLMs on-device processing, ensuring that your data remains on your device. This not only enhances security but also has the potential to minimize latency, contributing to a more efficient user experience.\n",
    "\n",
    "Generative AI models are a class of AI that can generate new data that resembles existing data. They include foundation models that can be fine-tuned for different tasks, Variational Autoencoders (VAEs) that reduce the dimensionality of data and Generative Adversarial Networks (GANs) that use competing networks to generate realistic samples. Other models include Transformer-based models that use attention mechanisms to model long-term text dependencies, diffusion models that address information decay by removing noise in the latent space, and multimodal large language models (LLMs) that integrate image and text understanding.\n",
    "In summary, generative AI models are a class of AI that can generate new data that resembles existing data. They include foundation models, VAEs, GANs, Transformer-based models, diffusion models, and multimodal LLMs.\n",
    "\n",
    "# Farshid PirahanSiah\n",
    "## Generative AI models on Edge with on-device training \n",
    "- CNN, RNN, Transformer-based models, LLMs (GPT, )\n",
    "- Generative AI models\n",
    "    - foundation model\n",
    "        - train on huge data -> adapt to applications\n",
    "        - GPT, CLIP, DALL-E, \n",
    "    - Variational Autoencoders (VAEs)\n",
    "        - rapidly reduce the dimensionality of samples\n",
    "        - input (image) -> encoder -> latent space -> decoder -> reconstructed output (image)\n",
    "        - use for \n",
    "            - image synthesis\n",
    "            - data compression\n",
    "            - anomaly detection\n",
    "        - **Standard VAEs**\n",
    "            - **Beta-VAE**: Balances latent capacity and reconstruction.\n",
    "        - **Conditional VAEs (CVAEs)**\n",
    "            - **Conditional Beta-VAE**: Adds conditional variables for targeted generation.\n",
    "        - **Hierarchical VAEs**\n",
    "            - **NVAE**: Deep hierarchy, residual connections for stable high-quality output.\n",
    "    - Generative Adversarial Networks (GANs)\n",
    "        - use competing networks to produce realistic samples\n",
    "        - generator ; discriminator\n",
    "        - GANs in finance, spaceGAN (geospatial data), styleGAN2 (create video game characters)\n",
    "        - **Convolutional GANs**\n",
    "            - **DCGAN**: Deep convolutional networks for stable, quality generation.\n",
    "        - **Style-based GANs**\n",
    "            - **StyleGAN**: Fine-grained style control, realistic faces.\n",
    "            - **StyleGAN2**: Redesigns normalization, reduces artifacts.\n",
    "        - **Progressive Growing GANs**\n",
    "            - **PGGAN**: Increases network size during training for high-resolution output.\n",
    "    - Transformer-based Models\n",
    "        - use attention mechanisms to model long-term text dependencies\n",
    "        - **Autoregressive Models**\n",
    "            - **GPT-3**: Predictive text generation based on input.\n",
    "            - **Image GPT (iGPT)**: Generates images pixel by pixel.\n",
    "        - **Multimodal Transformers**\n",
    "            - **DALL-E**: Generates images from text descriptions.\n",
    "    - Diffusion Models\n",
    "        - address information decay by removing noise in the latent space\n",
    "        - step 1: forward diffusion to add random noise to the data; \n",
    "        - step 2: reverse diffusion: turn the noise , recover data, generate the desired output\n",
    "        - **Basic Diffusion Models**\n",
    "            - **DDPM**: Generates data from noise by reversing diffusion.\n",
    "        - **Advanced Diffusion Techniques**\n",
    "            - **Improved DDPM**: More efficient, improved sampling.\n",
    "            - **Guided Diffusion**: Produces specific outputs with classifier guidance.\n",
    "    - Multimodal Large Language Models (LLMs)\n",
    "        - **CLIP**\n",
    "            - Integrates image and text understanding.\n",
    "        - **Perceiver IO**\n",
    "            - Handles audio, visual, text with a generalized architecture.\n",
    "        - **FLAVA**\n",
    "            - Self-supervised, unified model for vision, language, and multimodal tasks.\n",
    "- LLMOps: MLOps Tools: MLflow and Hugging Face    \n",
    "    - <span style=\"color: green; font-weight: bold; font-style: italic; white-space: nowrap;\"> ðŸ“š ðŸ¤” MLflow <span style=\"display: inline-block; animation: slide 2s infinite linear;\">â†’</span> </span>  <style>            @keyframes slide {            0% { transform: translateX(0); }            100% { transform:translateX(20px); }            }            </style>\n",
    " \n",
    "        - MLflow Tracking - Logs key metrics, parameters, models, and other artifacts when running ML code to monitor experiments\n",
    "        - MLflow Projects - Configurable standard format for organizing ML code to ensure consistency and reproducibility\n",
    "        - MLflow Models - Package ML model files with their dependencies so they can be deployed on diverse platforms\n",
    "        - pip install mlflow\n",
    "        - mlflow ui\n",
    "        - mlflow experiments create --experiment-name metrics-test ==> id 2\n",
    "        - MLFLOW_EXPERIMENT_ID=2 python test-mlflow.py\n",
    "        - load_model= mlflow.pyfunc.load_model(logged_model)\n",
    "        - mlflow run . -P filename=inputfile\n",
    "        - hugging face \n",
    "            - models, dataset, spaces\n",
    "            - streamlit, gradio\n",
    "            - pip install huggingface_hub\n",
    "            - huggingface-cli login\n",
    "            - git lfs install\n",
    "            - git clone <model url>\n",
    "            - \n",
    "        - \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Large Vision Models (LVMs)\n",
    "-  Stable Diffusion\n",
    "    - Why Stable Diffusion \n",
    "        - **Brainstorming and Ideation:** Rapidly generate images to explore various design layouts, scenes, or concepts. Itâ€™s an efficient tool for experimenting with creative ideas, such as different settings or stylized visual effects, providing more specific and customized placeholders than stock photos or traditional sketches.\n",
    "        - **Character Design:** Whether for video games, animations, or comic books, Stable Diffusion can help conceptualize and visualize characters, aiding in the development and refinement of unique designs.\n",
    "        - **Storyboarding:** Create visual narratives for video productions, movies, or advertisements. Stable Diffusion can generate scenes and sequences, facilitating the storytelling process and the visualization of cinematic ideas.\n",
    "        - **Communication:** Enhance presentations or proposals with high-quality visualizations. For office managers, interior designers, or event planners, itâ€™s a tool to convey ideas about space layouts, mood, atmosphere, and color schemes effectively, even if the images arenâ€™t perfect replicas.\n",
    "        - **Asset Generation:** Produce elements for compositing into photographs or artworks, create textures for 3D modeling, or develop backgrounds for motion graphics. Itâ€™s a versatile tool for augmenting traditional and digital art processes with unique visual components.\n",
    "        - **Fun and Exploration:** Beyond professional applications, Stable Diffusion offers a platform for fun and creative exploration. It encourages playful experimentation with images and concepts, often leading to unexpected and delightful results.  \n",
    "    - **Stable Diffusion**\n",
    "        - **Image Style**\n",
    "            - Blends photorealism and illustration\n",
    "            - Customizable styles via data models\n",
    "        - **Data Models**\n",
    "            - Open-source, community-driven enhancements\n",
    "            - Customizable for content safety or style\n",
    "        - **Accessibility**\n",
    "            - Free and open-source\n",
    "            - Standalone applications\n",
    "            - Integrates with software like Photoshop\n",
    "        - **Ethics**\n",
    "            - Training on diverse datasets\n",
    "            - Ethical considerations similar to competitors\n",
    "        - **Features**\n",
    "            - Image-to-image generation\n",
    "            - Custom training for specific recognition\n",
    "        - **Compared to Others**\n",
    "            - DALL-E and Midjourney focus on photorealism\n",
    "            - Adobe Firefly trained on ethical datasets\n",
    "            - Midjourney accessible via Discord\n",
    "            - Others require payment, Stable Diffusion is free\n",
    "        - **Community Impact**\n",
    "            - Vibrant open-source community\n",
    "            - Continuous improvement and feature additions\n",
    "        - **Technology**\n",
    "            - AI image generator revolution\n",
    "            - Different from traditional digital tools\n",
    "            - Democratizes sophisticated image generation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenai/led-large-16384-arxiv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# If you are running on a GPU, you can uncomment the device parameter to utilize it\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3523\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3524\u001b[0m     (\n\u001b[1;32m   3525\u001b[0m         model,\n\u001b[1;32m   3526\u001b[0m         missing_keys,\n\u001b[1;32m   3527\u001b[0m         unexpected_keys,\n\u001b[1;32m   3528\u001b[0m         mismatched_keys,\n\u001b[1;32m   3529\u001b[0m         offload_index,\n\u001b[1;32m   3530\u001b[0m         error_msgs,\n\u001b[0;32m-> 3531\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3536\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3538\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3539\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3542\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3543\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3549\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:3904\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   3896\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3897\u001b[0m         state_dict,\n\u001b[1;32m   3898\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3902\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   3903\u001b[0m     )\n\u001b[0;32m-> 3904\u001b[0m     error_msgs \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3905\u001b[0m     offload_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3907\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   3908\u001b[0m \n\u001b[1;32m   3909\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:628\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 628\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:626\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:626\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 626 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:626\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 626\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:622\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    620\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_param\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"allenai/led-large-16384-arxiv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# If you are running on a GPU, you can uncomment the device parameter to utilize it\n",
    "# pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "long_text = text\n",
    "generated_text = pipe(\n",
    "    long_text, \n",
    "    truncation=True, \n",
    "    max_length=1024,  # Adjusted to a higher value for potentially longer outputs\n",
    "    no_repeat_ngram_size=5, \n",
    "    num_beams=3, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Printing the generated text to see the output\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input ids are automatically padded from 2756 to 3072 to be a multiple of `config.attention_window`: 1024\n",
    "[{'generated_text': ' generative models are a class of artificial intelligence ( AI ) that can generate new data that resembles existing data. \\n they include foundation models that can be fine-tuned for different tasks , Variational Autoencoders (VAEs ) that reduce the dimensionality of data and Generative Adversarial Networks (GANs ) that use competing networks to generate realistic samples . \\n other models include Transformer-based models that use attention mechanisms to model long-term text dependencies , diffusion models that address information decay by removing noise in the latent space , and multimodal large language models (LLMs ) that integrate image and text understanding.    \\n generative models on Edge with on-device training - CNN, RNN , Transformer-based model , LLMs (GPT, )- Generative models on Edge  - foundation model , on-device training , on-device processing  - image synthesis , image compression , anomaly detection , data compression     * keywords : * generative models , image processing , image synthesis , image synthesis , data compression , data compression , anomaly detection '}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index\n",
    "#!pip install llama-index-embeddings-huggingface\n",
    "#!pip install llama-index-embeddings-instructor\n",
    "#!pip install llama-index-llms-ollama\n",
    "#!pip install llama-index\n",
    "from llama_index.core import SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "input_dir_path='/Users/farshid/farshid/pirahansiah.github.io/src/books/goal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleDirectoryReader' object has no attribute 'file_paths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Ensure each document is a tuple containing file path and text\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m docs \u001b[38;5;241m=\u001b[39m [(file_path, text) \u001b[38;5;28;01mfor\u001b[39;00m file_path, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_paths\u001b[49m, loader\u001b[38;5;241m.\u001b[39mcorpus)]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Enhance documents with metadata and define content formatting\u001b[39;00m\n\u001b[1;32m     33\u001b[0m enhanced_docs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleDirectoryReader' object has no attribute 'file_paths'"
     ]
    }
   ],
   "source": [
    "# 3new Tuesday, 2024\n",
    "from llama_index.core import SimpleDirectoryReader, PromptTemplate, VectorStoreIndex, Settings, Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.schema import MetadataMode\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Initialize settings for embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n",
    "\n",
    "# Function to extract and format metadata from file paths\n",
    "def extract_metadata(file_path):\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    return {\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"file_extension\": file_extension,\n",
    "        \"file_path\": file_path,\n",
    "    }\n",
    "\n",
    "# Load data from various file types and ensure metadata includes file paths\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=input_dir_path,\n",
    "    required_exts=[\".pdf\"],  # Add more extensions as needed\n",
    "    recursive=True,\n",
    "    # If necessary, extend SimpleDirectoryReader to include file path in the document object\n",
    ")\n",
    "docs = loader.load_data()\n",
    "\n",
    "# Ensure each document is a tuple containing file path and text\n",
    "docs = [(file_path, text) for file_path, text in zip(loader.file_paths, loader.corpus)]\n",
    "\n",
    "# Enhance documents with metadata and define content formatting\n",
    "enhanced_docs = []\n",
    "for file_path, text in docs:\n",
    "    metadata = extract_metadata(file_path)\n",
    "    document = Document(\n",
    "        text=text,\n",
    "        metadata=metadata,\n",
    "        excluded_llm_metadata_keys=[\"file_name\"],  # Exclude file name from LLM view\n",
    "        metadata_separator=\"::\",\n",
    "        metadata_template=\"{key}=>{value}\",\n",
    "        text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\"\n",
    "    )\n",
    "    enhanced_docs.append(document)\n",
    "    \n",
    "\n",
    "# Create vector store and index the documents with enhanced metadata handling\n",
    "index = VectorStoreIndex.from_documents(enhanced_docs)\n",
    "\n",
    "# Save the index for later use\n",
    "with open('index_data.pkl', 'wb') as file:\n",
    "    pickle.dump(index, file)\n",
    "\n",
    "# Example output of how metadata is seen by different models\n",
    "for doc in enhanced_docs[:1]:  # Show example for the first document\n",
    "    print(\n",
    "        \"The LLM sees this: \\n\",\n",
    "        doc.get_content(metadata_mode=MetadataMode.LLM),\n",
    "    )\n",
    "    print(\n",
    "        \"The Embedding model sees this: \\n\",\n",
    "        doc.get_content(metadata_mode=MetadataMode.EMBED),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new Tuesday, 2024\n",
    "from llama_index.core import SimpleDirectoryReader, PromptTemplate, VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import pickle\n",
    "\n",
    "# Initialize settings for embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n",
    "\n",
    "# Load data from various file types\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=input_dir_path,\n",
    "    required_exts=[\".pdf\", \".md\", \".txt\", \".html\"],  # Add more extensions as needed\n",
    "    recursive=True\n",
    ")\n",
    "docs = loader.load_data()\n",
    "\n",
    "# Create vector store and index the documents\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "# Save the index for later use\n",
    "with open('index_data.pkl', 'wb') as file:\n",
    "    pickle.dump(index, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, I'll take a step-by-step approach to answer your question.\n",
      "\n",
      "The query asks about \"methods for better reach to goal.\" Since we have a list of various goal-setting methodologies, let's review them:\n",
      "\n",
      "* OKRs (Objectives and Key Results): High-level objectives with measurable key results.\n",
      "* BHAG (Big Hairy Audacious Goals): Long-term, visionary goals.\n",
      "* Backward Goal Setting: Start with the ultimate goal and plan backward.\n",
      "* WIGs (Wildly Important Goals): Focus on a small number of crucial goals.\n",
      "* KPIs (Key Performance Indicators): Measure how effectively goals are achieved.\n",
      "* Goal Pyramid: Visual method using a hierarchical structure.\n",
      "* Vision Boards: Collage of images and words representing goals.\n",
      "* MTO (Minimum, Target, Outrageous): Three levels for each goal.\n",
      "* GROW Model: Goal, Reality, Options, Will; a coaching framework.\n",
      "* Action Plans: Detailed, step-by-step plans.\n",
      "* HARD Goals: Heartfelt, Animated, Required, Difficult.\n",
      "* S.M.A.R.T.E.R. Goals: SMART with Evaluated and Reviewed.\n",
      "* Stretch Goals: Extremely ambitious goals beyond current limits.\n",
      "\n",
      "Additionally, we have:\n",
      "\n",
      "* 5x5 Rule: Focus on five goals for at least five years.\n",
      "* Level 10 Life: Rate and set goals in different life areas.\n",
      "* Design Thinking: Empathize, define, ideate, prototype, test.\n",
      "* 4DX (Four Disciplines of Execution): Strategy execution framework.\n",
      "* Balance Wheel: Rate satisfaction in various life areas.\n",
      "* 3 P's (Positive, Present, Personal): Positive, present-tense, personalized goals.\n",
      "* SWOT Analysis: Strengths, Weaknesses, Opportunities, Threats.\n",
      "* The Golden Circle: Why, How, What.\n",
      "* The Rule of 3: Focus on three outcomes for different time frames.\n",
      "\n",
      "To summarize, these are the various methods for reaching your goals:\n",
      "\n",
      "* OKRs\n",
      "* BHAG\n",
      "* Backward Goal Setting\n",
      "* WIGs\n",
      "* KPIs\n",
      "* Goal Pyramid\n",
      "* Vision Boards\n",
      "* MTO\n",
      "* GROW Model\n",
      "* Action Plans\n",
      "* HARD Goals\n",
      "* S.M.A.R.T.E.R. Goals\n",
      "* Stretch Goals\n",
      "* 5x5 Rule\n",
      "* Level 10 Life\n",
      "* Design Thinking\n",
      "* 4DX\n",
      "* Balance Wheel\n",
      "* 3 P's\n",
      "* SWOT Analysis\n",
      "* The Golden Circle\n",
      "* The Rule of 3\n",
      "\n",
      "So, the answer is: these are the various methods for better reaching your goals.\n"
     ]
    }
   ],
   "source": [
    "# new Tuesday, 2024\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# setting up the llm\n",
    "llm = Ollama(model=\"llama3\", request_timeout=820.0) \n",
    "\n",
    "# ====== Setup a query engine on the index previously created ======\n",
    "Settings.llm = llm # specifying the llm to be used\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)\n",
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "response = query_engine.query('What are the methods for better reach to goal?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'StreamingResponse' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Page: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_number\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot available\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'StreamingResponse' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for result in response['results']:\n",
    "    print(f\"File: {result['metadata']['file_name']}, Page: {result['metadata'].get('page_number', 'Not available')}\")\n",
    "    print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, PromptTemplate\n",
    "\n",
    "# load data\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# ====== Create vector store and upload indexed data ======\n",
    "Settings.embed_model = embed_model # we specify the embedding model to be used\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "import pickle\n",
    "with open('index_data.pkl', 'wb') as file:\n",
    "    pickle.dump(index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('index_data.pkl', 'wb') as file:\n",
    "    pickle.dump(index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m query_engine\u001b[38;5;241m.\u001b[39mupdate_prompts({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_synthesizer:text_qa_template\u001b[39m\u001b[38;5;124m\"\u001b[39m: qa_prompt_tmpl})\n\u001b[1;32m     23\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhat are the methods for better reach to goal?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/base/response/schema.py:123\u001b[0m, in \u001b[0;36mStreamingResponse.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_txt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_gen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     response_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_gen:\n\u001b[1;32m    124\u001b[0m         response_txt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_txt \u001b[38;5;241m=\u001b[39m response_txt\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/llms/llm.py:111\u001b[0m, in \u001b[0;36mstream_chat_response_to_tokens.<locals>.gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TokenGen:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m chat_response_gen:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/core/llms/callbacks.py:150\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n\u001b[1;32m    149\u001b[0m     last_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f_return_val:\n\u001b[1;32m    151\u001b[0m         dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    152\u001b[0m             LLMChatInProgressEvent(\n\u001b[1;32m    153\u001b[0m                 messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m             )\n\u001b[1;32m    157\u001b[0m         )\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m cast(ChatResponse, x)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/llama_index/llms/ollama/base.py:147\u001b[0m, in \u001b[0;36mOllama.stream_chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    146\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line:\n\u001b[1;32m    149\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpx/_models.py:861\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    859\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_text():\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode(text):\n\u001b[1;32m    863\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpx/_models.py:848\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    846\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 848\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_bytes():\n\u001b[1;32m    849\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(byte_content)\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(text_content):\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    827\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    830\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpx/_models.py:883\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    880\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 883\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    884\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    885\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpx/_client.py:126\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpx/_transports/default.py:113\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 113\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    114\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:367\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:363\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpcore/_sync/http11.py:349\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpcore/_sync/http11.py:341\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 341\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    342\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpcore/_sync/http11.py:210\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    207\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# setting up the llm\n",
    "llm = Ollama(model=\"llama3\", request_timeout=820.0) \n",
    "\n",
    "# ====== Setup a query engine on the index previously created ======\n",
    "Settings.llm = llm # specifying the llm to be used\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)\n",
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "response = query_engine.query('What are the methods for better reach to goal?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context information, I can confidently answer that:\n",
      "\n",
      "SMART stands for Specific, Measurable, Achievable, Relevant, and Time-bound. It's a framework used to set clear and actionable goals, introduced by George T. Doran in 1981.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is SMART?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farshid\n"
     ]
    }
   ],
   "source": [
    "print('farshid')\n",
    "input_dir_path='/Users/farshid/farshid/pirahansiah.github.io/src/LLMs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911f69c3740c4daba9974d2e2690fe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/754 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68823032c35453eb032202f85250d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb086d7dd8b4383850fdd6a4f970573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cec6998a27f4a07aadcd227888fd6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8fc326599747fb995d396983f92509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6e0ad135ea45aba31eceb18b154f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bec3994a5bb45b2808e7a03f3becc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0c5cef08634f8d9598a1674e38e91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20063aca2ae7425383e88aa936dc9331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79489bda44d94a4cac70d89de1fe77d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61239a36e544fb7864e74557e7fc075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/0f/ef/0fefbdb29f9bf8c07cd5e2a1ff6ca195c1322623896ba5724adc8930e85cb6b6/02a36fb5d7b0574dc4c8da545089aac85b14a7972908293c11a63eca8ef46c0e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00002-of-00004.safetensors%3B+filename%3D%22model-00002-of-00004.safetensors%22%3B&Expires=1714019230&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDAxOTIzMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBmL2VmLzBmZWZiZGIyOWY5YmY4YzA3Y2Q1ZTJhMWZmNmNhMTk1YzEzMjI2MjM4OTZiYTU3MjRhZGM4OTMwZTg1Y2I2YjYvMDJhMzZmYjVkN2IwNTc0ZGM0YzhkYTU0NTA4OWFhYzg1YjE0YTc5NzI5MDgyOTNjMTFhNjNlY2E4ZWY0NmMwZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=hYzNzrHP-jtwd3qgfkUk-WclMWi4HZJMyKw5n4oIKQEKs6ZllRgpZcB2rJhioGRvHxngcRzfgPkUQ-HXeyBKMteAycLAf26NhtaxMweANk-Gqm0kKR36QwVWOkiJLsBlzEB%7EZ7mncj5G7gfQP0NqERAAItpO1rUfGRCVVOTXhoQzykm%7ELUhB61pQ2bYyQeTN3CEWUlGErbos2TN3-fycCvVub5FeMiwVwY0o-JuyX4eOLwcHnkhBYpb-hPTAfdhmbOONrkctvrua6GiK22oJ4pKr%7E8iH5tXxkPqeiAVYYULLOg2AWFrrvWaZsMXxplGOHo0a7KEaPj-4d8fC3AsRBg__&Key-Pair-Id=KCD77M1F0VK2B: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1c6063d9444d00a4b5536c4db2de32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:  95%|#########4| 4.66G/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f705b41bffc14362b799221b0dbfb55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1742e3a50004e7ea8a6865339b5ce56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6126f17a7a17428f8614db29e924732d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b2ed2379614c9b8fc7898af8938184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "microsoft/git-base-ft is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/microsoft/git-base-ft/resolve/main/preprocessor_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:1247\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1247\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:1624\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:402\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 402\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:426\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    425\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 426\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:320\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    312\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6625f2e4-5649a2a667a7f1437bbeffe1;eb6c3e65-7182-4adf-978b-c7543eec8020)\n\nRepository Not Found for url: https://huggingface.co/microsoft/git-base-ft/resolve/main/preprocessor_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m processor \u001b[38;5;241m=\u001b[39m LlavaNextProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/llava-v1.6-mistral-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m LlavaNextForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/llava-v1.6-mistral-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m---> 25\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mViTFeatureExtractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/git-base-ft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Create the HuggingFacePipeline for the \"llava\" model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFacePipeline(model\u001b[38;5;241m=\u001b[39mmodel, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilingual-image-to-text\u001b[39m\u001b[38;5;124m\"\u001b[39m, feature_extractor\u001b[38;5;241m=\u001b[39mfeature_extractor)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/image_processing_utils.py:205\u001b[0m, in \u001b[0;36mImageProcessingMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 205\u001b[0m image_processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(image_processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/image_processing_utils.py:334\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m image_processor_file \u001b[38;5;241m=\u001b[39m IMAGE_PROCESSOR_NAME\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     resolved_image_processor_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_processor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: microsoft/git-base-ft is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "# from llama_index.core import SimpleDirectoryReader\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.embeddings.huggingface import PromptTemplate, LLMPredictor, ServiceContext\n",
    "# from llama_index.embeddings import VectorStoreIndex, PromptTemplate, LLMPredictor, ServiceContext\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor\n",
    "\n",
    "# Load the \"llava\" model and feature extractor\n",
    "#model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/git-base-ft\")\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"microsoft/git-base-ft\")\n",
    "\n",
    "# Create the HuggingFacePipeline for the \"llava\" model\n",
    "llm = HuggingFacePipeline(model=model, task=\"multilingual-image-to-text\", feature_extractor=feature_extractor)\n",
    "\n",
    "# Create the LLMPredictor\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "\n",
    "# Set up the service context\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "# Load data\n",
    "loader = SimpleDirectoryReader(input_dir=input_dir_path, required_exts=[\".jpg\", \".png\"], recursive=True)\n",
    "docs = loader.load_data()\n",
    "\n",
    "# ====== Create vector store and upload indexed data ======\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True, use_auth_token=\"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\")\n",
    "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)\n",
    "\n",
    "# ====== Setup a query engine on the index previously created ======\n",
    "query_engine = index.as_query_engine(service_context=service_context, streaming=True, similarity_top_k=4)\n",
    "\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str, template_type=\"response_synthesizer\", response_synthesizer=\"text_qa\")\n",
    "\n",
    "query_engine.update_prompts({\"response_synthesizer\": qa_prompt_tmpl})\n",
    "\n",
    "response = query_engine.query('What are the methods for better reach to goal?')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(36350) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pirahansiah\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade llama_index\n",
    "#!pip install --upgrade llama_index langchain transformers\n",
    "#!huggingface-cli login\n",
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "llava-hf/llava-v1.6-mistral-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/llava-hf/llava-v1.6-mistral-7b/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:1247\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1247\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:1624\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1624\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:402\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 402\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/file_download.py:426\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    425\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 426\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:320\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    312\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m     )\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6625f39b-3c83f5b9051147244b3ae0bd;6b7de46d-00e7-461d-9c85-c8f7711b1886)\n\nRepository Not Found for url: https://huggingface.co/llava-hf/llava-v1.6-mistral-7b/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrich\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mprint\u001b[39m\n\u001b[1;32m      6\u001b[0m processor \u001b[38;5;241m=\u001b[39m LlavaNextProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava-hf/llava-v1.6-mistral-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaNextForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllava-hf/llava-v1.6-mistral-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# -hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#model.to(\"cuda:0\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# prepare image and text prompt, using the appropriate prompt template\u001b[39;00m\n\u001b[1;32m     12\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/modeling_utils.py:2899\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[1;32m   2716\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2728\u001b[0m ):\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;124;03m    Instantiate a pretrained pytorch model from a pre-trained model configuration.\u001b[39;00m\n\u001b[1;32m   2731\u001b[0m \n\u001b[1;32m   2732\u001b[0m \u001b[38;5;124;03m    The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;124;03m    the model, you should first set it back in training mode with `model.train()`.\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m \n\u001b[1;32m   2735\u001b[0m \u001b[38;5;124;03m    The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\u001b[39;00m\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;124;03m    pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\u001b[39;00m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;124;03m    task.\u001b[39;00m\n\u001b[1;32m   2738\u001b[0m \n\u001b[1;32m   2739\u001b[0m \u001b[38;5;124;03m    The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\u001b[39;00m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;124;03m    weights are discarded.\u001b[39;00m\n\u001b[1;32m   2741\u001b[0m \n\u001b[1;32m   2742\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;124;03m        pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\u001b[39;00m\n\u001b[1;32m   2744\u001b[0m \u001b[38;5;124;03m            Can be either:\u001b[39;00m\n\u001b[1;32m   2745\u001b[0m \n\u001b[1;32m   2746\u001b[0m \u001b[38;5;124;03m                - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\u001b[39;00m\n\u001b[1;32m   2747\u001b[0m \u001b[38;5;124;03m                - A path to a *directory* containing model weights saved using\u001b[39;00m\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;124;03m                  [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;124;03m                - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;124;03m                  this case, `from_tf` should be set to `True` and a configuration object should be provided as\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;124;03m                  `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\u001b[39;00m\n\u001b[1;32m   2752\u001b[0m \u001b[38;5;124;03m                  PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\u001b[39;00m\n\u001b[1;32m   2753\u001b[0m \u001b[38;5;124;03m                - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\u001b[39;00m\n\u001b[1;32m   2754\u001b[0m \u001b[38;5;124;03m                  `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\u001b[39;00m\n\u001b[1;32m   2755\u001b[0m \u001b[38;5;124;03m                  `True`.\u001b[39;00m\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;124;03m                - `None` if you are both providing the configuration and state dictionary (resp. with keyword\u001b[39;00m\n\u001b[1;32m   2757\u001b[0m \u001b[38;5;124;03m                  arguments `config` and `state_dict`).\u001b[39;00m\n\u001b[1;32m   2758\u001b[0m \u001b[38;5;124;03m        model_args (sequence of positional arguments, *optional*):\u001b[39;00m\n\u001b[1;32m   2759\u001b[0m \u001b[38;5;124;03m            All remaining positional arguments will be passed to the underlying model's `__init__` method.\u001b[39;00m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;124;03m        config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\u001b[39;00m\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;124;03m            Can be either:\u001b[39;00m\n\u001b[1;32m   2762\u001b[0m \n\u001b[1;32m   2763\u001b[0m \u001b[38;5;124;03m                - an instance of a class derived from [`PretrainedConfig`],\u001b[39;00m\n\u001b[1;32m   2764\u001b[0m \u001b[38;5;124;03m                - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\u001b[39;00m\n\u001b[1;32m   2765\u001b[0m \n\u001b[1;32m   2766\u001b[0m \u001b[38;5;124;03m            Configuration for the model to use instead of an automatically loaded configuration. Configuration can\u001b[39;00m\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;124;03m            be automatically loaded when:\u001b[39;00m\n\u001b[1;32m   2768\u001b[0m \n\u001b[1;32m   2769\u001b[0m \u001b[38;5;124;03m                - The model is a model provided by the library (loaded with the *model id* string of a pretrained\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;124;03m                  model).\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;124;03m                - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;124;03m                  save directory.\u001b[39;00m\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;124;03m                - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\u001b[39;00m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;124;03m                  configuration JSON file named *config.json* is found in the directory.\u001b[39;00m\n\u001b[1;32m   2775\u001b[0m \u001b[38;5;124;03m        state_dict (`Dict[str, torch.Tensor]`, *optional*):\u001b[39;00m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;124;03m            A state dictionary to use instead of a state dictionary loaded from saved weights file.\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \n\u001b[1;32m   2778\u001b[0m \u001b[38;5;124;03m            This option can be used if you want to create a model from a pretrained configuration but load your own\u001b[39;00m\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;124;03m            weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\u001b[39;00m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;124;03m            [`~PreTrainedModel.from_pretrained`] is not a simpler option.\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;124;03m        cache_dir (`Union[str, os.PathLike]`, *optional*):\u001b[39;00m\n\u001b[1;32m   2782\u001b[0m \u001b[38;5;124;03m            Path to a directory in which a downloaded pretrained model configuration should be cached if the\u001b[39;00m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;124;03m            standard cache should not be used.\u001b[39;00m\n\u001b[1;32m   2784\u001b[0m \u001b[38;5;124;03m        from_tf (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   2785\u001b[0m \u001b[38;5;124;03m            Load the model weights from a TensorFlow checkpoint save file (see docstring of\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;124;03m            `pretrained_model_name_or_path` argument).\u001b[39;00m\n\u001b[1;32m   2787\u001b[0m \u001b[38;5;124;03m        from_flax (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   2788\u001b[0m \u001b[38;5;124;03m            Load the model weights from a Flax checkpoint save file (see docstring of\u001b[39;00m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;124;03m            `pretrained_model_name_or_path` argument).\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;124;03m        ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   2791\u001b[0m \u001b[38;5;124;03m            Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m \u001b[38;5;124;03m            as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\u001b[39;00m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;124;03m            checkpoint with 3 labels).\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;124;03m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;124;03m            Whether or not to force the (re-)download of the model weights and configuration files, overriding the\u001b[39;00m\n\u001b[1;32m   2796\u001b[0m \u001b[38;5;124;03m            cached versions if they exist.\u001b[39;00m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;124;03m        resume_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;124;03m            Whether or not to delete incompletely received files. Will attempt to resume the download if such a\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;124;03m            file exists.\u001b[39;00m\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;124;03m        proxies (`Dict[str, str]`, *optional*):\u001b[39;00m\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;124;03m            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;124;03m            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;124;03m        output_loading_info(`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \u001b[38;5;124;03m            Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\u001b[39;00m\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;124;03m        local_files_only(`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;124;03m            Whether or not to only look at local files (i.e., do not try to download the model).\u001b[39;00m\n\u001b[1;32m   2807\u001b[0m \u001b[38;5;124;03m        token (`str` or `bool`, *optional*):\u001b[39;00m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;124;03m            The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;124;03m            the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\u001b[39;00m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*, defaults to `\"main\"`):\u001b[39;00m\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;124;03m            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[38;5;124;03m            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;124;03m            identifier allowed by git.\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \n\u001b[1;32m   2815\u001b[0m \u001b[38;5;124;03m            <Tip>\u001b[39;00m\n\u001b[1;32m   2816\u001b[0m \n\u001b[1;32m   2817\u001b[0m \u001b[38;5;124;03m            To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\u001b[39;00m\n\u001b[1;32m   2818\u001b[0m \n\u001b[1;32m   2819\u001b[0m \u001b[38;5;124;03m            </Tip>\u001b[39;00m\n\u001b[1;32m   2820\u001b[0m \n\u001b[1;32m   2821\u001b[0m \u001b[38;5;124;03m        mirror (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m \u001b[38;5;124;03m            Mirror source to accelerate downloads in China. If you are from China and have an accessibility\u001b[39;00m\n\u001b[1;32m   2823\u001b[0m \u001b[38;5;124;03m            problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\u001b[39;00m\n\u001b[1;32m   2824\u001b[0m \u001b[38;5;124;03m            Please refer to the mirror site for more information.\u001b[39;00m\n\u001b[1;32m   2825\u001b[0m \u001b[38;5;124;03m        _fast_init(`bool`, *optional*, defaults to `True`):\u001b[39;00m\n\u001b[1;32m   2826\u001b[0m \u001b[38;5;124;03m            Whether or not to disable fast initialization.\u001b[39;00m\n\u001b[1;32m   2827\u001b[0m \n\u001b[1;32m   2828\u001b[0m \u001b[38;5;124;03m            <Tip warning={true}>\u001b[39;00m\n\u001b[1;32m   2829\u001b[0m \n\u001b[1;32m   2830\u001b[0m \u001b[38;5;124;03m            One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\u001b[39;00m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;124;03m            4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\u001b[39;00m\n\u001b[1;32m   2832\u001b[0m \u001b[38;5;124;03m            [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\u001b[39;00m\n\u001b[1;32m   2833\u001b[0m \n\u001b[1;32m   2834\u001b[0m \u001b[38;5;124;03m            </Tip>\u001b[39;00m\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;124;03m        attn_implementation (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m   2836\u001b[0m \u001b[38;5;124;03m            The attention implementation to use in the model (if relevant). Can be any of `\"eager\"` (manual implementation of the attention), `\"sdpa\"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), or `\"flash_attention_2\"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `\"eager\"` implementation.\u001b[39;00m\n\u001b[1;32m   2837\u001b[0m \n\u001b[1;32m   2838\u001b[0m \u001b[38;5;124;03m        > Parameters for big model inference\u001b[39;00m\n\u001b[1;32m   2839\u001b[0m \n\u001b[1;32m   2840\u001b[0m \u001b[38;5;124;03m        low_cpu_mem_usage(`bool`, *optional*):\u001b[39;00m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;124;03m            Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\u001b[39;00m\n\u001b[1;32m   2842\u001b[0m \u001b[38;5;124;03m            This is an experimental feature and a subject to change at any moment.\u001b[39;00m\n\u001b[1;32m   2843\u001b[0m \u001b[38;5;124;03m        torch_dtype (`str` or `torch.dtype`, *optional*):\u001b[39;00m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;124;03m            Override the default `torch.dtype` and load the model under a specific `dtype`. The different options\u001b[39;00m\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;124;03m            are:\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \n\u001b[1;32m   2847\u001b[0m \u001b[38;5;124;03m            1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\u001b[39;00m\n\u001b[1;32m   2848\u001b[0m \u001b[38;5;124;03m              `dtype`, ignoring the model's `config.torch_dtype` if one exists. If not specified\u001b[39;00m\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;124;03m              - the model will get loaded in `torch.float` (fp32).\u001b[39;00m\n\u001b[1;32m   2850\u001b[0m \n\u001b[1;32m   2851\u001b[0m \u001b[38;5;124;03m            2. `\"auto\"` - A `torch_dtype` entry in the `config.json` file of the model will be\u001b[39;00m\n\u001b[1;32m   2852\u001b[0m \u001b[38;5;124;03m              attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\u001b[39;00m\n\u001b[1;32m   2853\u001b[0m \u001b[38;5;124;03m              the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\u001b[39;00m\n\u001b[1;32m   2854\u001b[0m \u001b[38;5;124;03m              using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\u001b[39;00m\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;124;03m              the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.\u001b[39;00m\n\u001b[1;32m   2856\u001b[0m \n\u001b[1;32m   2857\u001b[0m \u001b[38;5;124;03m            <Tip>\u001b[39;00m\n\u001b[1;32m   2858\u001b[0m \n\u001b[1;32m   2859\u001b[0m \u001b[38;5;124;03m            For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\u001b[39;00m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;124;03m            reach out to the authors and ask them to add this information to the model's card and to insert the\u001b[39;00m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;124;03m            `torch_dtype` entry in `config.json` on the hub.\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m \n\u001b[1;32m   2863\u001b[0m \u001b[38;5;124;03m            </Tip>\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \n\u001b[1;32m   2865\u001b[0m \u001b[38;5;124;03m        device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\u001b[39;00m\n\u001b[1;32m   2866\u001b[0m \u001b[38;5;124;03m            A map that specifies where each submodule should go. It doesn't need to be refined to each\u001b[39;00m\n\u001b[1;32m   2867\u001b[0m \u001b[38;5;124;03m            parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\u001b[39;00m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;124;03m            same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\u001b[39;00m\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;124;03m            like `1`) on which the model will be allocated, the device map will map the entire model to this\u001b[39;00m\n\u001b[1;32m   2870\u001b[0m \u001b[38;5;124;03m            device. Passing `device_map = 0` means put the whole model on GPU 0.\u001b[39;00m\n\u001b[1;32m   2871\u001b[0m \n\u001b[1;32m   2872\u001b[0m \u001b[38;5;124;03m            To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\u001b[39;00m\n\u001b[1;32m   2873\u001b[0m \u001b[38;5;124;03m            more information about each option see [designing a device\u001b[39;00m\n\u001b[1;32m   2874\u001b[0m \u001b[38;5;124;03m            map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\u001b[39;00m\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;124;03m        max_memory (`Dict`, *optional*):\u001b[39;00m\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;124;03m            A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\u001b[39;00m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;124;03m            GPU and the available CPU RAM if unset.\u001b[39;00m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;124;03m        offload_folder (`str` or `os.PathLike`, *optional*):\u001b[39;00m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;124;03m            If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\u001b[39;00m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;124;03m        offload_state_dict (`bool`, *optional*):\u001b[39;00m\n\u001b[1;32m   2881\u001b[0m \u001b[38;5;124;03m            If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\u001b[39;00m\n\u001b[1;32m   2882\u001b[0m \u001b[38;5;124;03m            RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\u001b[39;00m\n\u001b[1;32m   2883\u001b[0m \u001b[38;5;124;03m            `True` when there is some disk offload.\u001b[39;00m\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;124;03m        offload_buffers (`bool`, *optional*):\u001b[39;00m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;124;03m            Whether or not to offload the buffers with the model parameters.\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m        quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;124;03m            A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\u001b[39;00m\n\u001b[1;32m   2888\u001b[0m \u001b[38;5;124;03m            bitsandbytes, gptq). There may be other quantization-related kwargs, including `load_in_4bit` and\u001b[39;00m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;124;03m            `load_in_8bit`, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;124;03m            quantizations and not preferred. consider inserting all such arguments into quantization_config\u001b[39;00m\n\u001b[1;32m   2891\u001b[0m \u001b[38;5;124;03m            instead.\u001b[39;00m\n\u001b[1;32m   2892\u001b[0m \u001b[38;5;124;03m        subfolder (`str`, *optional*, defaults to `\"\"`):\u001b[39;00m\n\u001b[1;32m   2893\u001b[0m \u001b[38;5;124;03m            In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\u001b[39;00m\n\u001b[1;32m   2894\u001b[0m \u001b[38;5;124;03m            specify the folder name here.\u001b[39;00m\n\u001b[1;32m   2895\u001b[0m \u001b[38;5;124;03m        variant (`str`, *optional*):\u001b[39;00m\n\u001b[1;32m   2896\u001b[0m \u001b[38;5;124;03m            If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is\u001b[39;00m\n\u001b[1;32m   2897\u001b[0m \u001b[38;5;124;03m            ignored when using `from_tf` or `from_flax`.\u001b[39;00m\n\u001b[1;32m   2898\u001b[0m \u001b[38;5;124;03m        use_safetensors (`bool`, *optional*, defaults to `None`):\u001b[39;00m\n\u001b[0;32m-> 2899\u001b[0m \u001b[38;5;124;03m            Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\u001b[39;00m\n\u001b[1;32m   2900\u001b[0m \u001b[38;5;124;03m            is not installed, it will be set to `False`.\u001b[39;00m\n\u001b[1;32m   2901\u001b[0m \n\u001b[1;32m   2902\u001b[0m \u001b[38;5;124;03m        kwargs (remaining dictionary of keyword arguments, *optional*):\u001b[39;00m\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;124;03m            Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\u001b[39;00m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;124;03m            `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\u001b[39;00m\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;124;03m            automatically loaded:\u001b[39;00m\n\u001b[1;32m   2906\u001b[0m \n\u001b[1;32m   2907\u001b[0m \u001b[38;5;124;03m                - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\u001b[39;00m\n\u001b[1;32m   2908\u001b[0m \u001b[38;5;124;03m                  underlying model's `__init__` method (we assume all relevant updates to the configuration have\u001b[39;00m\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;124;03m                  already been done)\u001b[39;00m\n\u001b[1;32m   2910\u001b[0m \u001b[38;5;124;03m                - If a configuration is not provided, `kwargs` will be first passed to the configuration class\u001b[39;00m\n\u001b[1;32m   2911\u001b[0m \u001b[38;5;124;03m                  initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\u001b[39;00m\n\u001b[1;32m   2912\u001b[0m \u001b[38;5;124;03m                  corresponds to a configuration attribute will be used to override said attribute with the\u001b[39;00m\n\u001b[1;32m   2913\u001b[0m \u001b[38;5;124;03m                  supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\u001b[39;00m\n\u001b[1;32m   2914\u001b[0m \u001b[38;5;124;03m                  will be passed to the underlying model's `__init__` function.\u001b[39;00m\n\u001b[1;32m   2915\u001b[0m \n\u001b[1;32m   2916\u001b[0m \u001b[38;5;124;03m    <Tip>\u001b[39;00m\n\u001b[1;32m   2917\u001b[0m \n\u001b[1;32m   2918\u001b[0m \u001b[38;5;124;03m    Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\u001b[39;00m\n\u001b[1;32m   2919\u001b[0m \u001b[38;5;124;03m    use this method in a firewalled environment.\u001b[39;00m\n\u001b[1;32m   2920\u001b[0m \n\u001b[1;32m   2921\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[1;32m   2922\u001b[0m \n\u001b[1;32m   2923\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[1;32m   2924\u001b[0m \n\u001b[1;32m   2925\u001b[0m \u001b[38;5;124;03m    ```python\u001b[39;00m\n\u001b[1;32m   2926\u001b[0m \u001b[38;5;124;03m    >>> from transformers import BertConfig, BertModel\u001b[39;00m\n\u001b[1;32m   2927\u001b[0m \n\u001b[1;32m   2928\u001b[0m \u001b[38;5;124;03m    >>> # Download model and configuration from huggingface.co and cache.\u001b[39;00m\n\u001b[1;32m   2929\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\u001b[39;00m\n\u001b[1;32m   2930\u001b[0m \u001b[38;5;124;03m    >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\u001b[39;00m\n\u001b[1;32m   2931\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"./test/saved_model/\")\u001b[39;00m\n\u001b[1;32m   2932\u001b[0m \u001b[38;5;124;03m    >>> # Update configuration during loading.\u001b[39;00m\n\u001b[1;32m   2933\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True)\u001b[39;00m\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;124;03m    >>> assert model.config.output_attentions == True\u001b[39;00m\n\u001b[1;32m   2935\u001b[0m \u001b[38;5;124;03m    >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\u001b[39;00m\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;124;03m    >>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\u001b[39;00m\n\u001b[1;32m   2937\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\u001b[39;00m\n\u001b[1;32m   2938\u001b[0m \u001b[38;5;124;03m    >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\u001b[39;00m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;124;03m    >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", from_flax=True)\u001b[39;00m\n\u001b[1;32m   2940\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2941\u001b[0m \n\u001b[1;32m   2942\u001b[0m \u001b[38;5;124;03m    * `low_cpu_mem_usage` algorithm:\u001b[39;00m\n\u001b[1;32m   2943\u001b[0m \n\u001b[1;32m   2944\u001b[0m \u001b[38;5;124;03m    This is an experimental function that loads the model using ~1x model size CPU memory\u001b[39;00m\n\u001b[1;32m   2945\u001b[0m \n\u001b[1;32m   2946\u001b[0m \u001b[38;5;124;03m    Here is how it works:\u001b[39;00m\n\u001b[1;32m   2947\u001b[0m \n\u001b[1;32m   2948\u001b[0m \u001b[38;5;124;03m    1. save which state_dict keys we have\u001b[39;00m\n\u001b[1;32m   2949\u001b[0m \u001b[38;5;124;03m    2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\u001b[39;00m\n\u001b[1;32m   2950\u001b[0m \u001b[38;5;124;03m    3. after the model has been instantiated switch to the meta device all params/buffers that\u001b[39;00m\n\u001b[1;32m   2951\u001b[0m \u001b[38;5;124;03m    are going to be replaced from the loaded state_dict\u001b[39;00m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;124;03m    4. load state_dict 2nd time\u001b[39;00m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;124;03m    5. replace the params/buffers from the state_dict\u001b[39;00m\n\u001b[1;32m   2954\u001b[0m \n\u001b[1;32m   2955\u001b[0m \u001b[38;5;124;03m    Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\u001b[39;00m\n\u001b[1;32m   2956\u001b[0m \n\u001b[1;32m   2957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2958\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   2959\u001b[0m     from_tf \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: llava-hf/llava-v1.6-mistral-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from rich import print\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b\") # -hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True) \n",
    "#model.to(\"cuda:0\")\n",
    "\n",
    "# prepare image and text prompt, using the appropriate prompt template\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "prompt = \"[INST] <image>\\nWhat is shown in this image? Keep it brief[/INST]\"\n",
    "\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\")#.to(\"cuda:0\")\n",
    "\n",
    "# autoregressively complete prompt\n",
    "output = model.generate(**inputs, max_new_tokens=100)\n",
    "result = processor.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(50545) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/farshid/miniconda3/envs/rag/bin/python: Error while finding module specification for 'llava.serve.model_worker' (ModuleNotFoundError: No module named 'llava')\n"
     ]
    }
   ],
   "source": [
    "#!python -m llava.serve.controller --host 0.0.0.0 --port 10000\n",
    "!python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3 --model-base lmsys/vicuna-13b-v1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.1.8-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.3.0)\n",
      "Requirement already satisfied: certifi in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.4)\n",
      "Requirement already satisfied: idna in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.6)\n",
      "Requirement already satisfied: sniffio in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /Users/farshid/miniconda3/envs/rag/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.9.0)\n",
      "Downloading ollama-0.1.8-py3-none-any.whl (9.4 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/farshid/farshid/pirahansiah.github.io/src/LLMs/a.jpg']\n",
      "Empty DataFrame\n",
      "Columns: [image_file, description]\n",
      "Index: []\n",
      "\n",
      "Processing /Users/farshid/farshid/pirahansiah.github.io/src/LLMs/a.jpg\n",
      "\n",
      " The image is a screenshot of a social media post featuring a QR code. Below the QR code, there is text that reads:\n",
      "\n",
      "\"PhD. Computer Vision with LLM. Inventor. Consultants. Professional lead R&D.\"\n",
      "\n",
      "Additionally, at the bottom of the screenshot, we see an image of a person with the name \"Farhan Pathan Saihal.\" The post seems to be related to this individual's professional background and expertise in computer vision. The QR code is likely intended for viewers to scan and access more information about Farhan or to connect with him professionally. "
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from ollama import generate\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "# Load the DataFrame from a CSV file, or create a new one if the file doesn't exist\n",
    "def load_or_create_dataframe(filename):\n",
    "    if os.path.isfile(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['image_file', 'description'])\n",
    "    return df\n",
    "\n",
    "df = load_or_create_dataframe('image_descriptions.csv')\n",
    "\n",
    "def get_png_files(folder_path):\n",
    "    return glob.glob(f\"{folder_path}/*.jpg\")\n",
    "\n",
    "# get the list of image files in the folder yopu want to process\n",
    "image_files = get_png_files(\"/Users/farshid/farshid/pirahansiah.github.io/src/LLMs/\") #(\"./images\") \n",
    "image_files.sort()\n",
    "\n",
    "print(image_files[:3])\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# processing the images \n",
    "def process_image(image_file):\n",
    "    print(f\"\\nProcessing {image_file}\\n\")\n",
    "    with Image.open(image_file) as img:\n",
    "        with BytesIO() as buffer:\n",
    "            img.save(buffer, format='PNG')\n",
    "            image_bytes = buffer.getvalue()\n",
    "\n",
    "    full_response = ''\n",
    "    # Generate a description of the image  llava:13b-v1.6\n",
    "    for response in generate(model='llava:latest', \n",
    "                             prompt='describe this image and make sure to include anything notable about it (include text you see in the image):', \n",
    "                             images=[image_bytes], \n",
    "                             stream=True):\n",
    "        # Print the response to the console and add it to the full response\n",
    "        print(response['response'], end='', flush=True)\n",
    "        full_response += response['response']\n",
    "\n",
    "    # Add a new row to the DataFrame\n",
    "    df.loc[len(df)] = [image_file, full_response]\n",
    "\n",
    "\n",
    "for image_file in image_files:\n",
    "    if image_file not in df['image_file'].values:\n",
    "        process_image(image_file)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('image_descriptions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
