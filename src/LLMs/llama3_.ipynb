{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, SMART stands for Specific, Measurable, Achievable, Relevant, and Time-bound. It's a framework used to set goals that are clear, actionable, and simple. SMART goals provide a structured approach to setting objectives that are achievable and meaningful.\n"
     ]
    }
   ],
   "source": [
    "#!pip install llama-index\n",
    "#!pip install llama-index-embeddings-huggingface\n",
    "#!pip install llama-index-embeddings-instructor\n",
    "#!pip install llama-index-llms-ollama\n",
    "#!pip install llama-index\n",
    "import pickle\n",
    "from llama_index.core import SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding( model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n",
    "Settings.embed_model = embed_model \n",
    "llm = Ollama(model=\"llama3\", request_timeout=820.0) \n",
    "Settings.llm = llm # specifying the llm to be used\n",
    "with open('index_data.pkl', 'rb') as file:\n",
    "    index = pickle.load(file)\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=8)\n",
    "qa_prompt_tmpl_str = (\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information above I want you to think step by step to answer the query in a crisp manner, incase case you don't know the answer say 'I don't know!'.\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "            )\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "response = query_engine.query('What is SMART?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
